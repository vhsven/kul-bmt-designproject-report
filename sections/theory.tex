\section{Theoretical background}
\subsection{Features}
A 'feature' represents a characteristic of a voxel and its environment. Lists of
features for all the voxels in an image are used to describe the entire image.

\subsubsection{Feature extraction}
Feature extraction involves obtaining a list of feature from a resource -- an
image or a another dataset -- to describe that dataset in an accurate way which
allows it to be analysed by proper software.

In image processing a wide range of features can be extracted. Some features are
very trivial: the grey value or colour of pixels, the position of pixels, the
intensity of certain grey values, etc. Features also can be shape based:
template matching, blob detection, edge detection, detection of lines and
circles by means of a Hough transform, etc. Once certain objects are detected in
the image their area, curvature, size, etc. can be determined.

\subsubsection{Feature selection}
A problem that may arise when analysing a dataset based on list of features, is
that the amount of features is too high. Performing an analysis on large amounts
of datasets requires a large amount of memory and computational power.
Furthermore, a classification algorithm that is used to analyse the data may
overfit the trainingsdata and will not be able to generalisation anymore.
Therefore, a range of dimension reduction techniques can be applied to extract
the uncorrelated and most important features of a list. Examples of these
reduction techniques are principal component analysis and partial least squares
regression.

\subsubsection{Features in nodule detection} \label{sec:featureselection}
A non-exhaustive literature review revealed commonly used features used in
automatic nodule detection. \cite{tartar} primarily used morphological features:
area, perimeter, diameter, solidity, eccentricity, aspect ratio, compactness,
roundness, circularity and ellipticity. To select these features the minimum
Redundancy Maximum Relevance (mRMR) method was applied. Selecting relevant
features is import to improve the accuracy of the algorithm and to reduce the
processing time. \cite{mur} used 3D local image features which were calculated
per voxel: shape index and curvedness. \cite{chen} calculated the size, margins,
contours and internal characteristics of the candidate nodules. \cite{keshani}
used 2D stochastic features –grey level values and intensity values-  as well as
3D anatomical features to remove the bronchioles from the list of candidate
nodules. The features selected by \cite{teramoto} were area, surface area,
volume, CT value, convergence, diameter and overlapping area. The algorithm of
\cite{ozekes} implemented 3D features such as straightness, thickness, vertical
and horizontal widths, regularity and vertical and horizontal black pixel
ratios.

These features are calculated based on a prior segmentation of the nodules. On
the other hand, instead of doing a nodule segmentation first, a lot of features
can be calculated on the image itself based on greyvalues, intensities,
greyvalues in the neighbourhood, etc. Although this is not a very common
approach, these features can be generated without any pre-processing of the
image and can then be fed to a classifier to calculate a nodule-probability for
each voxel.


\subsection{Machine learning}
The research field of machine learning is dedicated to the automatically
learning of software in order to make accurate predictions based on past
observations \cite{mach}. This concept is of course very interesting when
detecting nodules as a vast amount of 'past observations' are available.
Classifiers are algorithms that classify given examples into a given set of
categories \cite{mach}. CAD systems which implement a classifier tend to
outperform the CAD systems which do not \cite{lee2010}. Therefore, the use of
classifiers is very common in this field of research and many classifiers have
been tested.

\subsubsection{Classifiers used in nodule detection systems}
\cite{caruana} compared the performance of a range of classifiers on eleven
binary classification problems. The supervised learning algorithms that were
used are SVM, NN, Naive Bayes, Memory-Based Learning, RF, Decision Trees, Bagged
Trees, Boosted Trees and Boosted Stumps. Prior to calibration Bagged Trees, RF
and NN perform the best on average across all test problems. After calibration
Boosted Trees outperformed all other methods. The performances of SVM, Boosted
Stumps and Naive Bayes were also dramatically improved by calibration. The
performance of RF was not increased significantly. Overall, \cite{caruana}
suggests calibrated boosted trees is the best learning algorithm. RF are close
second, followed by uncalibrated bagged trees, calibrated SVM and uncalibrated
NN. However, the training of Boosted Trees is inherently sequential which makes
it slower to implement than RF. Another problem may be the noisyness of the
data. Therefore, the choice and tuning of the parameters of the boosted trees
(depth of trees, amount of trees, etc.) algorithm should be done carefully and
this will take some time.

SVM are widely used in the development of nodule detection CAD systems
\cite{keshani, lee2010, ozekes}. According to \cite{ash} SVM even outperforms
RF. However, SVM has some disadvantages. First of all, the featues to be used
need to be determined in advance and there is no such thing as a standard
feature set (see also \autoref{sec:featureselection} ). In the ideal case these
features should all have the same dimensions and/or magnitudes. With SVM
problems may arise with noisy data and images are very often quite noisy. The
time complexity of SVM is $O(n^2)$ \cite{svmcompex}.

The best performing CAD system in the ANODE09 challenge, which is entirely
dedicated to comparing nodule detection CAD systems, was the ISI-CAD algorithm
\cite{ginneken}. ISI-CAD uses a k-nearest neighbour classifier to reduce the
amount of FPs which has the disadvantage that the features should all be of the
same magnitude in order to perform an optimal classification.

 
\subsubsection{Random Forests, an ensemble classifier }
Ensemble learners combine decisions of multiple classifiers to form an
integrated output \cite{lee2010}. The use of multiple learning algorithms at the
same time has the advantage that a better predictive performance is obtained
compared to the predictive performance demonstrated by each individual learning
algorithm separately.

RF is a relatively new classification method which has not been exhaustively
explored yet. ``Random Forests are a combination of tree predictors such that
each tree depends on the values of a random vector sampled independently and
with the same distribution for all trees in the forest. The generalisation error
for forests converges to a limit as the number of trees in the forest becomes
large.'' \cite[~p.5]{breiman} Therefore, RF is an ensemble learning method.
So given a test sample as the input, this input vector is put down each of the
trees, each tree gives a classification and the forest selects the
classification that has most votes.
Therefore, the output of the RF depends on the combination of results from all
individual trees. In this way a variance reduction is achieved and the output is
made more robust against noise \cite{breiman, lee2010, RFcompex}. RF has the
advantage that the set of features do not need to be known in advance as the
algorithm itself decides on itself which features to use. Therefore, a lot of
features can be generated at will and the algorithm itself will decide on using
them or not. The time complexity of RF is $O(n \log n)$ \cite{RFcompex} which
makes it more suitable than e.g. SVM for large datasets.

However, although the RF classifier might be a good choice for analysing large
datasets because of the fact that a large amount of features can be used and
because of the rather limited time complexity compared to other classifiers, the
amount of memory and computational power can still be a problem. Therefore, an
optimisation of the implementation of the RF classifier can be desirable. A
possible optimisation is the use of a cascaded classifier.

\subsubsection{Cascaded classifiers}
A cascaded classifier exists of several classifiers at different levels that are
concatenated. So it is a special case of an ensemble classifier. The cascaded
classifier uses all the information that is obtained in a previous level to
provide the classifier in the next level with additional info on the data. On a
lower level in a cascaded classifier the amount of features and the complexity
of the features that are used are lower.
For example, the first feature in a classifier used to classify the voxels of a
3D image into two classes -- nodules or non-nodules -- is the greyvalue of each
voxel. This is a feature we get ``for free'' as it is readily available without
performing any calculations. The classifier at the first level determines a
threshold to separate nodulevoxels from non-nodulevoxels based on their grey
values. Using this trivial feature a large part of the voxels can already be
eliminated as their grey value is too high or too low to be a nodulevoxel. The
features on the second level are more complex and therefore require more
computational power. However, this is not a problem anymore as the amount of
voxels to be processed was reduced after the first level. At the second level
again a number of voxels are eliminated. The number of levels, and therefore the
number of features, can be increased until the end results are satisfying: all
voxels are classified (correctly) into x amount of classes. However, classifying
all voxels in the correct class is not trivial and some voxels might be
classified in the wrong class. Which amount of false positives and false
negatives is acceptable is to be decided by the user.

The reason for using a cascaded classifier is that both memory and CPU time are
often limited. By discarding a certain amount of non-nodules at each level of
the classifier, it is not necessary to calculate all features for all nodules which saves memory and CPU time.

The performance of the classifier depends on the features that are implemented
on each level.
For datamining the general rule is the more the better. If a lot a features are
provided for each voxel the classifier can set more thresholds and is therefore
better able to separate different classes. We started off from this concept, but
very soon we had to deal with memory errors in Python. The code was therefore
optimised, but still we had to cut in the amount of features that were used in
the final version of the classifier as the amount of memory available remained a
problem during the whole project.









 





