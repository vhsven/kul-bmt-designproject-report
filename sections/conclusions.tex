\section{Conclusion}
CT technology has come to the point where high resolution images of the whole
chest can be obtained in a single breath hold. The resolution allows to detect
pulmonary nodules in an early stage and therefore CT scans become more and more
part of routine investigations. This causes a large increase in the workload of
the radiologists, who are only human and thus also prone to errors. Time
pressure and fatigue may lead to an increasing fraction of overlooked nodules.
Research showed however that pulmonary lung detection systems, that serve as a
second reader, can improve the performances of radiologists.
Therefore, the goal of this project was to develop an accurate, fast and
automated system to detect pulmonary nodules in CT scans.

A non-exhaustive literature review revealed that companies such as R2
Technology, Siemens, iCAD etc. already have invested in the development of
similar software. However, a satisfying allround software package does not exist
yet. The research is still ongoing to develop a system with 100\% sensitivity
and no false positives detection. One of the problems that arises here is that
there is no golden standard to measure the performance of the CAD system
against. Currently, the performances of CAD systems are compared with the
findings of one or more radiologists. Although the fraction of overlooked
nodules decreases when more radiologists cooperate when analysing a CT scan,
there is no guarantee that all nodules are delineated in a scan. This makes it
very hard to assess the performance of a automated nodule detection system.

There are three main schools of thought in the development of pulmonary nodule CAD
systems. The first group uses template matching to detect (a type of) nodules. A
second group performs a nodule segmentation by means of a series of
morphological operations, active contour modelling, etc. The third group applies
classification methods, possibly aided by clustering. As there is evidence
in literature that this method yields the best results, a more extensive
literature review was performed to select a proper classification method. It was
decided to use a cascaded Random Forest classifier as this type of classifiers
is not yet fully explored in this area of research. This provided the
opportunity to beat the state of the art in nodule detection algorithms.

Random Forests have many advantages. As an ensemble classifier it combines
decisions of multiple classifiers to form an integrated output. This way of
working has the advantage that a better predictive performance is obtained
compared to the predictive performance demonstrated by each individual learning
algorithm separately. Furthermore, Random Forests are rather robust against
noise compared with other classifier such as Support Vector Machines. Random
Forests also allow to use a lot of features, even if they have different orders
of magnitude, without increasing the time complexity too much. The features also
do not have to be known in advance. At the same time, the method does not
require a lot of parameter tuning. The only thing that has to be taken into
consideration is the depth of the trees as overfitting must be avoided in order
to maintain an algorithm that is able to generalise across datasets. The use of
a cascaded classifier is preferred as it limits the amount of CPU time and
memory storage.
 
The higher the level in the cascaded classifier, the more complex the features.
On the first level the grey values of the voxels were used to detect soft
tissue, to which class nodules belong. Although this first level is able to
eliminate the voxels outside the lungs, a lung segmentation was applied first to
reduce the amount of voxels to be processed. The reason for this were recurrent
memory errors when attempted otherwise. On the second level a blobdetector --
Laplacian filter -- and a distance map were implemented. On the third and fourth
level a 3D averaging algorithm was elaborated with two different
parametersettings. This 3D averaging allows to separate nodules from bronchioles
and bronchi by taking into account the presence of these structures in the
preceeding and/or succeeding slices. A lot of other features were implemented
and tested, but most of them were removed again from the final classifier as
they required a lot of processing time and did not perform accordingly. After
each level in the classifier a threshold was set to determine which voxels were
taken to the next level and which were to be discarded. These thresholds were
empirically determined, but can still be optimised. The training and the
validation of the algorithm were performed on 30 and 8 datasets respectively.

During the training of the algorithm a five-fold crossvalidation was performed
to determine the optimal set of parameters for the Random Forest algorithm. It
was decided to take into account the optimal number of minimum samples per leaf
to keep the algorithm from overfitting. An accuracy level of 98,2\% was achieved
in the last level of the cascaded classifier.

The validation of the optimised classifier showed 100\% sensitivity, but also
indicated the amount of false positives still has to be significantly reduced.
The processing time of 10 minutes per scan is not extremely satisfying, but one
has to take into account that Python is an interpreted language which makes it
inherently slower than for instance C++. Transferring the code to a compiled
language will speed up the process. The aim is to be able to process one scan
within a few minutes. There is no need for faster processing as a radiologist
will not need the results earlier, but it should not take much longer either.

The amount of false positives per scan (4279,43 FP per scan) has to be reduced.
A first step to achieve this is the optimisation of the thresholds in the
algorithm. A more accurate setting of the thresholds will also be possible if the
algorithm is trained on a larger amount of scans. On top of that, the clustering
strategy might have to be revisited. Another possibility is the implementation
of more features. However, there are two constraints here. The first
one is trivial: the more features, the longer it will take to process the scan.
A second consideration that has to be made is the fact that not all features are
suitable for our approach. We only selected features that could be extracted
from the image without performing any nodule segmentation in advance to make the
algorithm faster an more robust. This nodule segmentation step would allow to
use a wider range of features, but could also introduce errors in the algorithm.
Furthermore, developing a nodule segmentation is not a trivial thing to do.

%ook niet overdrijven, 64bit doet geen wonderen..
Although the use of the features and the code of the cascaded classifier were
optimised considerably, one of the largest problems we encountered were the
recurring out-of-memory errors. In the beginning of the project we decided to
use Python 2.7.6 in the 32-bit version as it is typically more stable than the
64-bit version. This showed to be a bad choice later on as we had to spend a lot
of time and efforts on the optimisation of the code concerning memory storage
and computational power. This also prevented us from implementing a lot of
features as we intended to do. The first concept was to calculate a range of
features and let the Random Forest algorithm then decide on which and how many
features to use in the final classifier. Because of the memory errors we had to
carefully select the features ourselves by visual assessment of the probability
images that were generated at each level in the classifier, rather than letting
RF figure out the most interesting features from a large pool.

Some other things to be done differently in the future is limiting the amount
of time spent on reviewing the literature, limiting the amount of time spent
on optimising the threshold values and performing the implementation of the
features in a different way. In the beginning of the project we implemented a
lot of features, which was also the aim before we encountered the memory
errors, but a lot of them proved to be useless or required too much processing
time. Instead, we should have tried some features, trained the algorithm and
make decisions on the type and amount of feautures based on these small
experiments.

Besides the suggestions for improvement mentioned above, we have some other
recommendations for future research. First of all, it would be interesting to
not only detect the nodules, but also classify them as benign or malignant. In
order to do this the grey value intensity gradient inside the nodule could be a
helpful feature. In order to implement this feature, the nodule voxels should be
clustered first. Also classifying the nodules accoring to the type of nodule
(juxta-vascular; pleural tail; well-circumscribed; juxta-pleural) would be
interesing as the type may already be an indication for the probability of
malignancy of a nodule. As the type of nodule was not available in the
annotations, it was not possible for us to implement this in the classifier.
An interesting comparison that can be made is whether the subtelty
that is assigned by the radiologist is comparible with the probability provided
by the algorithm. And may the algorithm overlook the same nodules as one
radiologist when assessing the CT scan?

Finally, if all optimalisations are performed the algorithm can be implemented
in MeVisLab as a stand-alone modules which can easily be used by radiologists or
researchers. To assess the performance of the optimised algorithm it can be
validated on the dataset of the ANODE09 challenge where it can be compared with
the results of other studies.