\section{Conclusion}
CT technology has come to the point where high resolution images of the whole
chest can be obtained in a single breath hold. The resolution allows to detect
pulmonary nodules in an early stage and therefore CT scans become more and more
part of routine investigations. This causes a large increase in the workload of
the radiologists, who are only human and thus also prone to errors. Time
pressure and fatigue may lead to an increasing fraction of overlooked nodules.
Research showed however that pulmonary lung detection systems, that serve as a
second reader, can improve the performances of radiologists.
Therefore, the goal of this project was to develop an accurate, fast and
automated system to detect pulmonary nodules in CT scans.

A non-exhaustive literature review revealed that companies such as R2
Technology, Siemens, iCAD etc. already have invested in the development of
similar software. However, a satisfying allround software package does not exist
yet. The research is still ongoing to develop a system with 100\% sensitivity
and no false positives detection. On of the problems that arises here is that
there is no golden standard to measure the performance of the CAD system
against. Currently, the performances of CAD systems are compared with the
findings of one or more radiologists. Although the fraction of overlooked
nodules decreases when more radiologists cooperate when analysing a CT scan,
there is no guarantee that all nodules are delineated in a scan. This makes it
very hard to assess the performance of a automated nodule detection system.

There are three main lines of thought in the development of pulmonary nodule CAD
systems. The first group uses template matching to detect (a type of) nodules. A
second group performs a nodule segmentation by means of a series of
morphological operations, active contour modelling, etc. The third group applies
classification methods, whether or not aided by clustering. As there is evidence
in literature that this method yields the best results, a more extensive
literature review was performed to select a proper classification method. It was
decided to use a cascaded Random Forest classifier as this type of classifiers
is not yet fully explored in this area of research. This provided the
opportunity to beat the state of the art in nodule detection algorithms.

Random Forests have many advantages. As an ensemble classifier it combines
decisions of multiple classifiers to form an integrated output. This way of
working has the advantage that a better predictive performance is obtained
compared to the predictive performance demonstrated by each individual learning
algorithm separately. Furthermore, Random Forests are rather robust against
noise compared with other classifier such as Support Vector Machines. Random
Forests also allow to use a lot of features, even if they have different orders
of magnitude, without increasing the time complexity too much. The features also
do not have to be known in advance. At the same time, the method does not
require a lot of parameter tuning. The only thing that has to be taken into
consideration is the depth of the trees as overfitting must be avoided in order
to maintain an algorithm that is able to generalise across datasets. The use of
a cascaded classifier is preferred as it limits the amount of CPU time and
memory storage.
 
The higher the level in the cascaded classifier, the more complex the features.
On the first level the greyvalues of the voxels were used to separate nodules
form non-nodules. Although this first level is able to eliminate the voxels
outside the lungs, a lung segmentation was applied first to reduce the amount of
voxels to be processed. The reason for this were recurrent memory errors. On the
second level a blobdetector -- Laplacian filter -- and a distance map were
implemented. On the third and fourth level a 3D averaging algorithm was
elaborated with two different parametersettings. This 3D averaging allows to
separate nodules from bronchioles and bronchi by taking into account the
presence of these structures in the preceeding and/or succeeding slices. A lot
of other features were implemented and tested, but most of them were removed
again from the final classifier as they required a lot of processing time and
did not perform accordingly. After each level in the classifier a treshold was
set to determine which voxels were taken to the next level and which were to be
discarded. These thresholds were also empirically determined, but can still be
optimised. The training and the validation of the algorithm were performed on 30
and 8 scans respectively. These scans remained after eliminating the annotations
which contained 1-voxel nodules. The pixel size of the scans varied between
0,586 and 0,963 mm and the slice thickness varied between 1,25 or 2,50 mm. This
means the diameter of the annotated 1-voxel micronodules was less than 1 mm.
These nodules are difficult to detect for a radiologist, especially if they are
hidden in a maze of vessels of the same magnitude. Therefore, some databases do
not even require the radiologist to mark such small findings. In the LIDC/IDRI
database there were no limitations set on the diameters of the nodules to be
marked. However, the chance the radiologists marked every single micronodule is
very small. The training of the CAD algorithm could therefore be ruined due to
these incomplete annotations. Therefore, the datasets containing only 1-voxel
micronodules and the annotations of 1-voxel nodules in the remaining datasets
were removed. Furthermore, focussing on the detection of these micronodules
would have put a limit to the level of thresholding that could be performed
during the classification of the datasets and would have prevented us from
eliminating a substantial part of the false positive findings.

During the training of the algorithm a five-fold crossvalidation was performed
to determine the optimal set of parameters for the Random Forest algorithm. It
was decided to take into account the optimal number of minimum samples per leaf
to keep the algorithm from overfitting. An accuracy level of 98,2\% was achieved
in the last level of the cascaded classifier.

The validation of the optimised classifier showed 100\% sensitivity, but also
indicated the amount of false positives still has to be reduced. The processing
time of 10 minutes per scan is not extremely satisfying, but one has to take
into account that Python is an interpreted language which makes it inherently
slower than for instance C++. Transferring the code to a compiled language will
speed up the process. The aim is to be able to process one scan within a few
minutes. There is no need for faster processing as a radiologist will not need
the results earlier, but it should not take longer than a few minutes.

The amount of false positives per scan (4357 FP per scan) has to be reduced. A
first step to achieve this is the optimisation of the thresholds in the
algorithm. A more accurate setting of the tresholds will also be possible if the
algorithm is trained on a larger amount of scans. Another possibility is the
implementation of more and/or other features. However, there are two contraints
here. The first one is trivial: the more features, the longer it will take to
process the scan. A second consideration that has to be made is the fact that
not all features are suitable for our approach. We only selected features that
could be extracted from the image without performing any nodule segmentation in
advance to make the algorithm faster an more robust. This nodule segmentation
step would allow to use a wider range of features, but could also introduce
errors in the algorithm. Furthermore, developing a nodule segmentation is not a
trivial thing to do.

Although the use of the features and the code of the cascaded classifier was
optimised as much as possible, one of the largest problems we encountered was
the ever returning memory errors. In the beginning of the project we decided to
use Python 2.7.6 in the 32-bit version as it is more stabile than the 64-bit
version. This showed to be a fatal choice later on as we had to spend a lot of
time and efforts on the optimalisation of the code concerning memory storage and
computational power. This also prevented us from implementing a lot of features
as we intended to do. The first concept was to calculate a range of features and
let the Random Forest algorithm then decide on which and how many features to
use in the final classifier. Because of the memory errors we had to select the
features ourselves by visual assessment of the probability images that were
generated at each level in the classifier.

Some other things to be done differently in the future is limiting the amount
of time spend on reviewing the literature, limiting the amount of time spend
on optimising the thresholdvalues and performing the implementation of the
features in a different way. In the beginning of the project we implemented a
lot of features, which was also the aim before we encountered the memory
errors, but a lot of them showed to be useless and/or require to much processing
time. Instead, we should have tried some features, trained the algorithm and
make decisions on the type and amount of feautures based on these small
experiments.

Besides the suggestions for improvement mentioned above, we have some other
recommendations for future research. First of all, it would be interesting to
not only detect the nodules, but also classify them as benign or malignant. In
order to do this the grey value intensity gradient inside the nodule could be a
helpful feature. In order to implement this feature, the nodulevoxels should be
clustered first. Also classifying the nodules accoring to the type of nodule
(juxta-vascular; pleural tail; well-circumscribed; juxta-pleural) would be
interesing as the type may already be an indication for the probability of
malignancy of a nodule. As the type of nodule was not available in the
annotations, it was not possible for us to implement this in the classifier.
An interesting comparison that can be made is whether the subtelty
that is assigned by the radiologist is comparible with the probability provided
by the algorithm. And may the algorithm overlook the same nodules as one
radiologist when assessing the CT scan?

Finally, if all optimalisations are
performed the algorithm can be implemented in MeVisLab (or a workstation) as a
stand-alone modules which can easily be used by radiologists or researchers. To
assess the performance of the optimised algorithm it can be validated on the
dataset of the ANODE09 challenge where it can be compared with the results of
other studies.

